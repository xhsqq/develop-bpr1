"""
Training script for Multimodal Recommender on Amazon datasets
ä½¿ç”¨çœŸå®Amazonæ•°æ®è®­ç»ƒï¼Œå…¨åº“è¯„ä¼°ï¼Œæ— è´Ÿé‡‡æ ·
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.tensorboard import SummaryWriter
import argparse
import os
import json
import yaml
from tqdm import tqdm
from typing import Dict, Optional
import numpy as np
from datetime import datetime
import math

from models.multimodal_recommender import MultimodalRecommender
from data.dataloader import get_dataloaders
from utils.evaluation import FullLibraryEvaluator, get_train_items_per_user


def load_config(config_path: str) -> Dict:
    """
    ä»YAMLé…ç½®æ–‡ä»¶åŠ è½½é…ç½®
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def merge_config_with_args(config: Dict, args: argparse.Namespace) -> argparse.Namespace:
    """
    åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼‰
    
    Args:
        config: ä»YAMLåŠ è½½çš„é…ç½®
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        åˆå¹¶åçš„å‚æ•°
    """
    # å‚æ•°åæ˜ å°„ï¼šconfig -> args
    param_mapping = {
        'learning_rate': 'lr',
        'warmup_epochs': 'warmup_epochs',
        'weight_decay': 'weight_decay',
        'batch_size': 'batch_size',
        'epochs': 'epochs',
        'eval_interval': 'eval_interval',
        'dropout': 'dropout'
    }
    
    # æ¨¡å‹é…ç½®
    if 'model' in config:
        model_config = config['model']
        for key, value in model_config.items():
            setattr(args, key, value)
            print(f"  âœ“ Set model.{key} = {value}")
    
    # è®­ç»ƒé…ç½®
    if 'training' in config:
        train_config = config['training']
        for key, value in train_config.items():
            if key == 'loss_weights':
                # å¤„ç†æŸå¤±æƒé‡
                for loss_key, loss_value in value.items():
                    loss_arg = f'alpha_{loss_key}' if not loss_key.startswith('alpha_') else loss_key
                    setattr(args, loss_arg, loss_value)
                    print(f"  âœ“ Set {loss_arg} = {loss_value}")
            elif key == 'early_stopping':
                # å¤„ç†æ—©åœé…ç½®
                if 'patience' in value:
                    args.early_stopping_patience = value['patience']
                if 'min_delta' in value:
                    args.early_stopping_min_delta = value['min_delta']
            elif key in param_mapping:
                # ä½¿ç”¨æ˜ å°„åçš„å‚æ•°åï¼ˆé…ç½®æ–‡ä»¶ä¼˜å…ˆçº§é«˜äºé»˜è®¤å€¼ï¼‰
                arg_name = param_mapping[key]
                setattr(args, arg_name, value)
                print(f"  âœ“ Set {arg_name} = {value}")
            else:
                # å…¶ä»–å‚æ•°ç›´æ¥è®¾ç½®ï¼ˆå¦‚æœargsä¸­å·²æœ‰è¯¥å±æ€§ï¼Œä¹Ÿè¦†ç›–ï¼‰
                setattr(args, key, value)
                print(f"  âœ“ Set {key} = {value}")
    
    # æ•°æ®é…ç½®
    if 'data' in config:
        data_config = config['data']
        for key, value in data_config.items():
            if hasattr(args, key):
                setattr(args, key, value)
                print(f"  âœ“ Set data.{key} = {value}")
    
    # æ—¥å¿—é…ç½®
    if 'logging' in config:
        logging_config = config['logging']
        if 'use_tensorboard' in logging_config:
            args.use_tensorboard = logging_config['use_tensorboard']
        if 'log_dir' in logging_config:
            args.log_dir = logging_config['log_dir']
        if 'exp_name' in logging_config and logging_config['exp_name']:
            args.exp_name = logging_config['exp_name']
        if 'save_dir' in logging_config:
            args.save_dir = logging_config['save_dir']
    
    # æ¶ˆèå®éªŒé…ç½®
    if 'ablation' in config:
        ablation_config = config['ablation']
        for key, value in ablation_config.items():
            setattr(args, f'ablation_{key}', value)
    
    return args


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=10, min_delta=0.001, mode='max'):
        """
        Args:
            patience: å®¹å¿çš„epochæ•°
            min_delta: æœ€å°æ”¹è¿›å¹…åº¦
            mode: 'max' æˆ– 'min'ï¼Œè¡¨ç¤ºæŒ‡æ ‡è¶Šå¤§è¶Šå¥½è¿˜æ˜¯è¶Šå°è¶Šå¥½
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        
    def __call__(self, score):
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        
        Returns:
            True if should stop, False otherwise
        """
        if self.best_score is None:
            self.best_score = score
            return False
        
        if self.mode == 'max':
            improved = score > self.best_score + self.min_delta
        else:
            improved = score < self.best_score - self.min_delta
            
        if improved:
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                return True
        
        return False


def get_training_phase(epoch: int) -> str:
    """
    ç¡®å®šå½“å‰è®­ç»ƒé˜¶æ®µ
    
    Phase 1 (0-10): ç»„ä»¶é¢„è®­ç»ƒ - ä¸“æ³¨é‡æ„ï¼Œå†»ç»“å› æœæ¨¡å—
    Phase 2 (10-30): è”åˆå¾®è°ƒ - å¹³è¡¡æ‰€æœ‰æŸå¤±
    Phase 3 (30+): ç«¯åˆ°ç«¯è®­ç»ƒ - ä¸“æ³¨æ¨èä»»åŠ¡
    """
    if epoch < 10:
        return 'phase1'
    elif epoch < 30:
        return 'phase2'
    else:
        return 'phase3'


def adjust_training_strategy(
    model: nn.Module,
    optimizer: optim.Optimizer,
    epoch: int,
    phase: str,
    initial_lr: float
) -> Dict[str, float]:
    """
    æ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´ç­–ç•¥ï¼šæ¨¡å—å†»ç»“/è§£å†»ã€æŸå¤±æƒé‡ã€å­¦ä¹ ç‡
    
    Returns:
        æ–°çš„æŸå¤±æƒé‡å­—å…¸
    """
    if phase == 'phase1':
        print(f"\n{'='*60}")
        print(f"ğŸ“ Phase 1: Component Pre-training (Epoch {epoch+1}/10)")
        print(f"   ç­–ç•¥: å†»ç»“å› æœæ¨¡å—ï¼Œä¸“æ³¨è§£è€¦è¡¨å¾å’Œå¤šå…´è¶£å­¦ä¹ ")
        print(f"{'='*60}")
        
        # å†»ç»“å› æœæ¨æ–­æ¨¡å—
        if hasattr(model, 'causal_inference'):
            for param in model.causal_inference.parameters():
                param.requires_grad = False
        
        # è°ƒæ•´æŸå¤±æƒé‡ - ä¸“æ³¨äºé‡æ„
        loss_weights = {
            'alpha_recon': 1.0,      # ä¸»è¦ä¼˜åŒ–é‡æ„
            'alpha_causal': 0.0,     # ç¦ç”¨å› æœæŸå¤±
            'alpha_diversity': 0.1,  # ä¿æŒå¤šæ ·æ€§
            'alpha_orthogonality': 0.1  # ä¿æŒæ­£äº¤æ€§
        }
        
    elif phase == 'phase2':
        if epoch == 10:  # åˆšè¿›å…¥phase2
            print(f"\n{'='*60}")
            print(f"ğŸ“ Phase 2: Joint Fine-tuning (Epoch {epoch-9}/20)")
            print(f"   ç­–ç•¥: è§£å†»æ‰€æœ‰æ¨¡å—ï¼Œå¹³è¡¡æ‰€æœ‰æŸå¤±")
            print(f"{'='*60}")
            
            # è§£å†»å› æœæ¨æ–­æ¨¡å—
            if hasattr(model, 'causal_inference'):
                for param in model.causal_inference.parameters():
                    param.requires_grad = True
        
        # å¹³è¡¡çš„æŸå¤±æƒé‡
        loss_weights = {
            'alpha_recon': 0.2,
            'alpha_causal': 0.1,   # é€æ¸å¼•å…¥å› æœæŸå¤±
            'alpha_diversity': 0.05,
            'alpha_orthogonality': 0.05
        }
        
    else:  # phase3
        if epoch == 30:  # åˆšè¿›å…¥phase3
            print(f"\n{'='*60}")
            print(f"ğŸ“ Phase 3: End-to-End Training (Epoch {epoch-29})")
            print(f"   ç­–ç•¥: ä¸“æ³¨æ¨èä»»åŠ¡ï¼Œè¾…åŠ©æŸå¤±æœ€å°åŒ–")
            print(f"{'='*60}")
            
            # é™ä½æ‰€æœ‰æ¨¡å—çš„å­¦ä¹ ç‡
            for param_group in optimizer.param_groups:
                param_group['lr'] = param_group['lr'] * 0.2
        
        # ä»¥æ¨èä»»åŠ¡ä¸ºä¸»
        loss_weights = {
            'alpha_recon': 0.01,
            'alpha_causal': 0.001,
            'alpha_diversity': 0.001,
            'alpha_orthogonality': 0.001
        }
    
    # æ›´æ–°æ¨¡å‹çš„æŸå¤±æƒé‡
    for key, value in loss_weights.items():
        if hasattr(model, key):
            setattr(model, key, value)
    
    return loss_weights


def check_gradient_health(model: nn.Module, batch_idx: int) -> Dict[str, float]:
    """
    æ£€æŸ¥æ¢¯åº¦å¥åº·çŠ¶æ€ï¼Œè¿”å›å„æ¨¡å—çš„æ¢¯åº¦ç»Ÿè®¡
    
    Returns:
        å„æ¨¡å—çš„æ¢¯åº¦èŒƒæ•°å­—å…¸
    """
    gradient_stats = {
        'item_embedding': 0.0,
        'disentangled': 0.0,
        'quantum': 0.0,
        'causal': 0.0,
        'other': 0.0,
        'nan_count': 0,
        'inf_count': 0
    }
    
    for name, param in model.named_parameters():
        if param.grad is None:
            continue
            
        grad_norm = param.grad.norm().item()
        
        # æ£€æŸ¥å¼‚å¸¸æ¢¯åº¦
        if torch.isnan(param.grad).any():
            gradient_stats['nan_count'] += 1
            print(f"âš ï¸  Batch {batch_idx}: NaN gradient in {name}")
            param.grad.zero_()
            continue
            
        if torch.isinf(param.grad).any():
            gradient_stats['inf_count'] += 1
            print(f"âš ï¸  Batch {batch_idx}: Inf gradient in {name}")
            param.grad.zero_()
            continue
        
        # åˆ†ç±»ç»Ÿè®¡
        if 'item_embedding' in name:
            gradient_stats['item_embedding'] += grad_norm
        elif 'disentangled' in name:
            gradient_stats['disentangled'] += grad_norm
        elif 'quantum' in name:
            gradient_stats['quantum'] += grad_norm
        elif 'causal' in name:
            gradient_stats['causal'] += grad_norm
        else:
            gradient_stats['other'] += grad_norm
    
    return gradient_stats


def train_epoch(
    model: nn.Module,
    dataloader,
    optimizer: optim.Optimizer,
    device: str,
    epoch: int,
    phase: str = None
) -> Dict[str, float]:
    """è®­ç»ƒä¸€ä¸ªepoch - æ”¯æŒæ¸è¿›å¼è®­ç»ƒç­–ç•¥"""
    model.train()

    total_loss = 0
    total_rec_loss = 0
    total_dis_loss = 0
    total_div_loss = 0
    total_orth_loss = 0
    total_causal_loss = 0
    
    # æ¢¯åº¦ç›‘æ§
    total_rec_grad = 0
    total_aux_grad = 0
    grad_samples = 0

    pbar = tqdm(dataloader, desc=f'Epoch {epoch}')

    for batch_idx, batch in enumerate(pbar):
        # ç§»åŠ¨åˆ°è®¾å¤‡
        item_ids = batch['item_ids'].to(device)
        target_items = batch['target_items'].to(device)
        seq_lengths = batch['seq_lengths'].to(device)
        multimodal_features = {
            k: v.to(device) for k, v in batch['multimodal_features'].items()
        }
        
        # â­ è´Ÿé‡‡æ ·æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        candidate_items = batch.get('candidate_items')
        labels = batch.get('labels')
        if candidate_items is not None:
            candidate_items = candidate_items.to(device)
        if labels is not None:
            labels = labels.to(device)

        # å‰å‘ä¼ æ’­
        outputs = model(
            item_ids=item_ids,
            multimodal_features=multimodal_features,
            seq_lengths=seq_lengths,
            target_items=target_items,
            candidate_items=candidate_items,  # â­ è´Ÿé‡‡æ ·å€™é€‰ç‰©å“
            labels=labels,  # â­ æ ‡ç­¾
            return_loss=True,
            return_explanations=False
        )

        loss = outputs['loss']
        
        # â­ NaNæ£€æµ‹ï¼šä¸€æ—¦å‘ç°lossä¸ºNaNç«‹å³ç»ˆæ­¢å¹¶æŠ¥å‘Š
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"\n{'='*80}")
            print(f"ğŸš¨ FATAL ERROR: Loss became {'NaN' if torch.isnan(loss) else 'Inf'} at batch {batch_idx}")
            print(f"{'='*80}")
            print(f"Loss breakdown:")
            for key in ['recommendation_loss', 'disentangled_loss', 'diversity_loss', 'orthogonality_loss', 'causal_loss']:
                if key in outputs:
                    val = outputs[key]
                    print(f"  {key}: {val}")
            print(f"\nModel state:")
            print(f"  temperature: {model.temperature.item():.6f}")
            print(f"  kl_anneal_factor: {model.kl_anneal_factor:.6f}")
            print(f"\nThis indicates a critical numerical instability.")
            print(f"Please check the model architecture or reduce learning rate.")
            raise RuntimeError("Training failed due to NaN/Inf loss")

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # â­ æ¢¯åº¦å¥åº·æ£€æŸ¥ï¼ˆè£å‰ªå‰ï¼‰
        if batch_idx % 50 == 0:
            grad_health = check_gradient_health(model, batch_idx)
            
            rec_grad_norm_before = 0
            aux_grad_norm_before = 0
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    if 'recommendation_head' in name or 'item_embedding' in name:
                        rec_grad_norm_before += grad_norm
                    else:
                        aux_grad_norm_before += grad_norm
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # æ¢¯åº¦ç›‘æ§ï¼ˆæ¯50ä¸ªbatchè®°å½•ä¸€æ¬¡ï¼‰- è£å‰ªå
        if batch_idx % 50 == 0:
            rec_grad_norm_after = 0
            aux_grad_norm_after = 0
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    if 'recommendation_head' in name or 'item_embedding' in name:
                        rec_grad_norm_after += grad_norm
                    else:
                        aux_grad_norm_after += grad_norm
            
            # è®°å½•è£å‰ªåçš„æ¢¯åº¦ï¼ˆç”¨äºè®¡ç®—å¹³å‡å€¼ï¼‰
            total_rec_grad += rec_grad_norm_after
            total_aux_grad += aux_grad_norm_after
            grad_samples += 1
            
            # å¦‚æœæ¢¯åº¦è¢«ä¸¥é‡è£å‰ªï¼Œæ‰“å°è­¦å‘Š
            if aux_grad_norm_before > 100:  # å¦‚æœè¾…åŠ©æ¢¯åº¦è¿‡å¤§
                print(f"\nâš ï¸  Batch {batch_idx}: Gradient clipping applied!")
                print(f"   Before: rec={rec_grad_norm_before:.2f}, aux={aux_grad_norm_before:.2f}")
                print(f"   After:  rec={rec_grad_norm_after:.2f}, aux={aux_grad_norm_after:.2f}")
            
            # å¦‚æœæ£€æµ‹åˆ°å¼‚å¸¸æ¢¯åº¦
            if grad_health['nan_count'] > 0 or grad_health['inf_count'] > 0:
                print(f"\nâš ï¸  Batch {batch_idx}: Detected {grad_health['nan_count']} NaN and {grad_health['inf_count']} Inf gradients (å·²æ¸…é›¶)")
        
        optimizer.step()

        # è®°å½•æŸå¤±
        total_loss += loss.item()
        total_rec_loss += outputs['recommendation_loss'].item()

        dis_loss = outputs['disentangled_loss']
        if isinstance(dis_loss, torch.Tensor):
            dis_loss = dis_loss.item()
        total_dis_loss += dis_loss

        div_loss = outputs['diversity_loss']
        if isinstance(div_loss, torch.Tensor):
            div_loss = div_loss.item()
        total_div_loss += div_loss
        
        orth_loss = outputs.get('orthogonality_loss', 0.0)
        if isinstance(orth_loss, torch.Tensor):
            orth_loss = orth_loss.item()
        total_orth_loss += orth_loss
        
        causal_loss = outputs.get('causal_loss', 0.0)
        if isinstance(causal_loss, torch.Tensor):
            causal_loss = causal_loss.item()
        total_causal_loss += causal_loss

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'rec': f'{outputs["recommendation_loss"].item():.4f}',
            'cau': f'{causal_loss:.4f}'
        })

    # è®¡ç®—å¹³å‡æŸå¤±
    num_batches = len(dataloader)
    metrics = {
        'loss': total_loss / num_batches,
        'rec_loss': total_rec_loss / num_batches,
        'dis_loss': total_dis_loss / num_batches,
        'div_loss': total_div_loss / num_batches,
        'orth_loss': total_orth_loss / num_batches,
        'causal_loss': total_causal_loss / num_batches
    }
    
    # æ·»åŠ æ¢¯åº¦ç›‘æ§ä¿¡æ¯
    if grad_samples > 0:
        avg_rec_grad = total_rec_grad / grad_samples
        avg_aux_grad = total_aux_grad / grad_samples
        metrics['rec_grad_norm'] = avg_rec_grad
        metrics['aux_grad_norm'] = avg_aux_grad
        print(f"\nğŸ“Š æ¢¯åº¦ç›‘æ§: rec_grad={avg_rec_grad:.2f}, aux_grad={avg_aux_grad:.2f}, ratio={avg_rec_grad/(avg_aux_grad+1e-8):.2f}")

    return metrics


def main():
    parser = argparse.ArgumentParser(description='Train Multimodal Recommender on Amazon')

    # æ•°æ®å‚æ•°
    parser.add_argument('--category', type=str, default='beauty',
                       choices=['beauty', 'games', 'sports'],
                       help='Amazon dataset category')
    parser.add_argument('--data_dir', type=str, default='data/processed')
    parser.add_argument('--max_seq_length', type=int, default=50)
    parser.add_argument('--use_text_features', action='store_true',
                       help='Use text features (slower)')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--hidden_dim', type=int, default=256)
    parser.add_argument('--item_embed_dim', type=int, default=128)
    parser.add_argument('--disentangled_dim', type=int, default=64)
    parser.add_argument('--num_interests', type=int, default=4)
    parser.add_argument('--quantum_state_dim', type=int, default=128)

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=256)
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--lr', type=float, default=1e-4)  # é™ä½å­¦ä¹ ç‡ä»3e-4åˆ°1e-4ï¼Œé˜²æ­¢NaN
    parser.add_argument('--warmup_epochs', type=int, default=5)  # æ·»åŠ Warmupï¼Œå‰5ä¸ªepochçº¿æ€§å¢é•¿
    parser.add_argument('--weight_decay', type=float, default=1e-5)

    # æŸå¤±æƒé‡ v0.7.0 - æç®€ç‰ˆï¼Œåˆç†èŒƒå›´ â­â­â­
    # é…åˆç®€åŒ–çš„æŸå¤±å‡½æ•°è®¾è®¡
    parser.add_argument('--alpha_recon', type=float, default=0.1, help='è§£è€¦è¡¨å¾æŸå¤±æƒé‡')
    parser.add_argument('--alpha_causal', type=float, default=0.5, help='å› æœæ¨æ–­æŸå¤±æƒé‡')
    parser.add_argument('--alpha_diversity', type=float, default=0.1, help='å¤šæ ·æ€§æŸå¤±æƒé‡')
    parser.add_argument('--alpha_orthogonality', type=float, default=0.1, help='æ­£äº¤æ€§æŸå¤±æƒé‡')

    # è¯„ä¼°å‚æ•°
    parser.add_argument('--eval_interval', type=int, default=5)
    parser.add_argument('--filter_train_items', action='store_true',
                       help='Filter training items during evaluation')

    # é…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str, default=None,
                       help='Path to YAML config file (overrides defaults)')
    
    # TensorBoardæ—¥å¿—
    parser.add_argument('--use_tensorboard', action='store_true',
                       help='Enable TensorBoard logging')
    parser.add_argument('--log_dir', type=str, default='logs',
                       help='TensorBoard log directory')
    parser.add_argument('--exp_name', type=str, default=None,
                       help='Experiment name for logging')
    
    # æ¶ˆèå®éªŒå‚æ•°
    parser.add_argument('--ablation_no_disentangled', action='store_true',
                       help='Ablation: disable disentangled representation')
    parser.add_argument('--ablation_no_causal', action='store_true',
                       help='Ablation: disable causal inference')
    parser.add_argument('--ablation_no_quantum', action='store_true',
                       help='Ablation: disable quantum-inspired encoder')
    parser.add_argument('--ablation_no_multimodal', action='store_true',
                       help='Ablation: use only item embeddings (no multimodal features)')
    parser.add_argument('--ablation_text_only', action='store_true',
                       help='Ablation: use only text features')
    parser.add_argument('--ablation_image_only', action='store_true',
                       help='Ablation: use only image features')
    
    # å…¶ä»–
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--save_dir', type=str, default='checkpoints')

    args = parser.parse_args()
    
    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
    if args.config:
        print(f"\n{'='*80}")
        print(f"Loading config from {args.config}")
        print('='*80)
        config = load_config(args.config)
        args = merge_config_with_args(config, args)
        print('='*80)
        print("âœ“ Config loaded and merged successfully")
        print('='*80 + '\n')

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)

    # ç”Ÿæˆå®éªŒåç§°
    if args.exp_name is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        ablation_suffix = ""
        if args.ablation_no_disentangled:
            ablation_suffix += "_no_dis"
        if args.ablation_no_causal:
            ablation_suffix += "_no_cau"
        if args.ablation_no_quantum:
            ablation_suffix += "_no_qua"
        if args.ablation_no_multimodal:
            ablation_suffix += "_no_mm"
        if args.ablation_text_only:
            ablation_suffix += "_text"
        if args.ablation_image_only:
            ablation_suffix += "_image"
        args.exp_name = f"{args.category}_{timestamp}{ablation_suffix}"
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = os.path.join(args.save_dir, args.exp_name)
    os.makedirs(save_dir, exist_ok=True)
    
    # åˆå§‹åŒ–TensorBoard
    writer = None
    if args.use_tensorboard:
        log_path = os.path.join(args.log_dir, args.exp_name)
        os.makedirs(log_path, exist_ok=True)
        writer = SummaryWriter(log_path)
        print(f"âœ“ TensorBoard logging enabled: {log_path}")
        print(f"  Run: tensorboard --logdir={args.log_dir}")

    # ä¿å­˜é…ç½®
    config_save_path = os.path.join(save_dir, 'config.json')
    with open(config_save_path, 'w') as f:
        json.dump(vars(args), f, indent=4)
    print(f"âœ“ Config saved to: {config_save_path}")

    print("\n" + "=" * 80)
    print(f"Training on Amazon {args.category.upper()} dataset")
    print(f"Experiment: {args.exp_name}")
    print("=" * 80)
    print(f"Device: {args.device}")
    
    # æ‰“å°æ¶ˆèå®éªŒè®¾ç½®
    ablation_info = []
    if args.ablation_no_disentangled:
        ablation_info.append("âŒ Disentangled Representation")
    if args.ablation_no_causal:
        ablation_info.append("âŒ Causal Inference")
    if args.ablation_no_quantum:
        ablation_info.append("âŒ Quantum-Inspired Encoder")
    if args.ablation_no_multimodal:
        ablation_info.append("âŒ Multimodal Features")
    if args.ablation_text_only:
        ablation_info.append("ğŸ“ Text Only")
    if args.ablation_image_only:
        ablation_info.append("ğŸ–¼ï¸  Image Only")
    
    if ablation_info:
        print("\nğŸ”¬ Ablation Study:")
        for info in ablation_info:
            print(f"  {info}")
    
    print("=" * 80 + "\n")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("Loading data...")
    train_loader, valid_loader, test_loader, dataset_info = get_dataloaders(
        category=args.category,
        data_dir=args.data_dir,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        max_seq_length=args.max_seq_length,
        use_text_features=args.use_text_features,
        num_negatives=0  # â­ å›é€€ï¼šç¦ç”¨è´Ÿé‡‡æ ·
    )

    num_users = dataset_info['num_users']
    num_items = dataset_info['num_items']

    print(f"Dataset: {num_users} users, {num_items} items")
    print(f"Train: {dataset_info['train_size']} samples")
    print(f"Valid: {dataset_info['valid_size']} samples")
    print(f"Test: {dataset_info['test_size']} samples\n")

    # æ¨¡æ€ç»´åº¦é…ç½® (æ ‡å‡†å¤šæ¨¡æ€: æ–‡æœ¬ + å›¾åƒ)
    modality_dims = {
        'text': 768,    # BERT-base / RoBERTa-base
        'image': 2048   # ResNet50
    }

    # åˆ›å»ºæ¨¡å‹
    print("Creating model...")
    model = MultimodalRecommender(
        modality_dims=modality_dims,
        disentangled_dim=args.disentangled_dim,
        num_disentangled_dims=3,
        num_interests=args.num_interests,
        quantum_state_dim=args.quantum_state_dim,
        hidden_dim=args.hidden_dim,
        item_embed_dim=args.item_embed_dim,
        num_items=num_items,
        max_seq_length=args.max_seq_length,
        alpha_recon=args.alpha_recon,
        alpha_causal=args.alpha_causal,
        alpha_diversity=args.alpha_diversity,
        alpha_orthogonality=args.alpha_orthogonality,
        use_quantum_computing=False
    ).to(args.device)

    num_params = sum(p.numel() for p in model.parameters())
    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model parameters: {num_params / 1e6:.2f}M ({num_trainable / 1e6:.2f}M trainable)")

    # â­ ä¼˜åŒ–å™¨ - ä¸åŒæ¨¡å—ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
    print("\n" + "=" * 80)
    print("ğŸ”§ Creating optimizer with differentiated learning rates:")
    
    param_groups = []
    
    # 1. Item Embedding - é«˜å­¦ä¹ ç‡ï¼ˆæ¨èä»»åŠ¡æ ¸å¿ƒï¼‰
    item_emb_params = []
    if hasattr(model, 'item_embedding'):
        item_emb_params = list(model.item_embedding.parameters())
    param_groups.append({
        'params': item_emb_params,
        'lr': args.lr * 3.0,  # 3å€åŸºç¡€å­¦ä¹ ç‡ï¼ˆä»2xæé«˜ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼‰
        'weight_decay': args.weight_decay,
        'name': 'item_embedding'
    })
    print(f"  âœ“ Item Embedding: lr={args.lr * 3.0:.2e}, {len(item_emb_params)} params")
    
    # 2. Disentangled Module - ä¸­ç­‰å­¦ä¹ ç‡
    disentangled_params = []
    if hasattr(model, 'disentangled_representation'):
        disentangled_params = list(model.disentangled_representation.parameters())
    param_groups.append({
        'params': disentangled_params,
        'lr': args.lr,
        'weight_decay': args.weight_decay * 2,  # æ›´å¼ºçš„æ­£åˆ™åŒ–
        'name': 'disentangled'
    })
    print(f"  âœ“ Disentangled Module: lr={args.lr:.2e}, {len(disentangled_params)} params")
    
    # 3. Quantum Encoder - ä¸­ç­‰å­¦ä¹ ç‡
    quantum_params = []
    if hasattr(model, 'quantum_interest_encoder'):
        quantum_params = list(model.quantum_interest_encoder.parameters())
    param_groups.append({
        'params': quantum_params,
        'lr': args.lr * 0.5,
        'weight_decay': args.weight_decay,
        'name': 'quantum'
    })
    print(f"  âœ“ Quantum Encoder: lr={args.lr * 0.5:.2e}, {len(quantum_params)} params")
    
    # 4. Causal Module - ä½å­¦ä¹ ç‡ï¼ˆå¤æ‚æ¨¡å—ï¼‰
    causal_params = []
    if hasattr(model, 'causal_inference'):
        causal_params = list(model.causal_inference.parameters())
    param_groups.append({
        'params': causal_params,
        'lr': args.lr * 0.2,  # ä½å­¦ä¹ ç‡ï¼ˆä»0.1xæé«˜åˆ°0.2xï¼‰
        'weight_decay': args.weight_decay,
        'name': 'causal'
    })
    print(f"  âœ“ Causal Module: lr={args.lr * 0.2:.2e}, {len(causal_params)} params")
    
    # 5. å…¶ä»–å‚æ•° - åŸºç¡€å­¦ä¹ ç‡
    param_ids = set()
    for pg in param_groups:
        param_ids.update(id(p) for p in pg['params'])
    
    other_params = [p for p in model.parameters() if id(p) not in param_ids]
    param_groups.append({
        'params': other_params,
        'lr': args.lr,
        'weight_decay': args.weight_decay * 0.5,
        'name': 'others'
    })
    print(f"  âœ“ Other Parameters: lr={args.lr:.2e}, {len(other_params)} params")
    
    optimizer = optim.AdamW(param_groups)
    print("=" * 80 + "\n")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ (Warmup + Cosine Annealing) â­
    def lr_lambda(current_epoch):
        """Warmup + Cosineè¡°å‡"""
        if current_epoch < args.warmup_epochs:
            # Warmupé˜¶æ®µ: ä»1e-5çº¿æ€§å¢é•¿åˆ°base_lr
            return (current_epoch + 1) / args.warmup_epochs
        else:
            # Cosineè¡°å‡é˜¶æ®µ
            progress = (current_epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)
            return 0.5 * (1.0 + math.cos(math.pi * progress))
    
    scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)
    
    # æ—©åœæœºåˆ¶ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
    patience = getattr(args, 'early_stopping_patience', 10)
    min_delta = getattr(args, 'early_stopping_min_delta', 0.001)
    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta, mode='max')

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = FullLibraryEvaluator(
        num_items=num_items,
        k_list=[5, 10, 20, 50]
    )

    # è·å–è®­ç»ƒé›†ç‰©å“ï¼ˆç”¨äºè¿‡æ»¤è¯„ä¼°ï¼‰
    if args.filter_train_items:
        print("Building train item filters...")
        train_items_per_user = get_train_items_per_user(train_loader.dataset)
        print(f"âœ“ Built filters for {len(train_items_per_user)} users\n")
    else:
        train_items_per_user = None

    # è®­ç»ƒå¾ªç¯
    best_ndcg = 0.0
    best_epoch = 0

    print("\n" + "=" * 80)
    print("ğŸš€ Starting Progressive Training...")
    print("   Phase 1 (Epoch 1-10): Component Pre-training")
    print("   Phase 2 (Epoch 11-30): Joint Fine-tuning")
    print("   Phase 3 (Epoch 31+): End-to-End Training")
    print("=" * 80 + "\n")

    for epoch in range(1, args.epochs + 1):
        print(f"Epoch {epoch}/{args.epochs}")
        print("-" * 80)
        
        # â­ ç¡®å®šè®­ç»ƒé˜¶æ®µå¹¶è°ƒæ•´ç­–ç•¥
        phase = get_training_phase(epoch - 1)  # epochä»1å¼€å§‹ï¼Œè½¬æ¢ä¸º0-based
        if epoch == 1 or (epoch - 1) in [10, 30]:  # é˜¶æ®µåˆ‡æ¢æ—¶æ‰“å°ç­–ç•¥
            loss_weights = adjust_training_strategy(
                model, optimizer, epoch - 1, phase, args.lr
            )
        
        # â­ KLé€€ç«ï¼šå‰20ä¸ªepoché€æ¸å¢åŠ KLæƒé‡
        # é¿å…VAEåéªŒåå¡Œé—®é¢˜
        kl_anneal_epochs = 20
        if epoch <= kl_anneal_epochs:
            model.kl_anneal_factor = min(1.0, epoch / kl_anneal_epochs)
            if epoch == 1:
                print(f"ğŸ”¥ KL Annealing enabled: factor will increase from 0.05 to 1.0 over {kl_anneal_epochs} epochs")
            if epoch % 5 == 0:
                print(f"   KL anneal factor: {model.kl_anneal_factor:.3f}")
        else:
            model.kl_anneal_factor = 1.0
        
        # è®­ç»ƒ
        train_metrics = train_epoch(model, train_loader, optimizer, args.device, epoch, phase)

        print(f"\nTraining metrics:")
        for key, value in train_metrics.items():
            print(f"  {key}: {value:.4f}")
        
        # TensorBoard: è®°å½•è®­ç»ƒæŒ‡æ ‡
        if writer:
            for key, value in train_metrics.items():
                writer.add_scalar(f'Train/{key}', value, epoch)
            writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)

        # éªŒè¯
        if epoch % args.eval_interval == 0 or epoch == args.epochs:
            print(f"\nValidating...")

            if train_items_per_user is not None:
                valid_metrics = evaluator.evaluate_with_filter(
                    model, valid_loader, train_items_per_user, args.device
                )
            else:
                valid_metrics = evaluator.evaluate(
                    model, valid_loader, args.device
                )

            print(f"\nValidation metrics:")
            for key, value in valid_metrics.items():
                print(f"  {key}: {value:.4f}")
            
            # TensorBoard: è®°å½•éªŒè¯æŒ‡æ ‡
            if writer:
                for key, value in valid_metrics.items():
                    writer.add_scalar(f'Valid/{key}', value, epoch)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if valid_metrics['NDCG@10'] > best_ndcg:
                best_ndcg = valid_metrics['NDCG@10']
                best_epoch = epoch

                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'train_metrics': train_metrics,
                    'valid_metrics': valid_metrics,
                    'args': vars(args)
                }, os.path.join(save_dir, 'best_model.pt'))

                print(f"\nâœ“ Saved best model (NDCG@10: {best_ndcg:.4f})")
            
            # æ—©åœæ£€æŸ¥
            if early_stopping(valid_metrics['NDCG@10']):
                print(f"\nâœ‹ Early stopping triggered! No improvement for {early_stopping.patience} evaluations.")
                print(f"Best NDCG@10: {best_ndcg:.4f} at epoch {best_epoch}")
                break

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

        print(f"\nLearning rate: {optimizer.param_groups[0]['lr']:.6f}")
        print("=" * 80 + "\n")

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹
    print("\n" + "=" * 80)
    print("Testing best model...")
    print("=" * 80 + "\n")

    # åŠ è½½æœ€ä½³æ¨¡å‹
    checkpoint = torch.load(os.path.join(save_dir, 'best_model.pt'))
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"Loaded best model from epoch {checkpoint['epoch']}")

    # æµ‹è¯•
    if train_items_per_user is not None:
        test_metrics = evaluator.evaluate_with_filter(
            model, test_loader, train_items_per_user, args.device
        )
    else:
        test_metrics = evaluator.evaluate(
            model, test_loader, args.device
        )

    print(f"\nTest metrics:")
    for key, value in test_metrics.items():
        print(f"  {key}: {value:.4f}")

    # ä¿å­˜æµ‹è¯•ç»“æœ
    results = {
        'exp_name': args.exp_name,
        'best_epoch': best_epoch,
        'valid_metrics': checkpoint['valid_metrics'],
        'test_metrics': test_metrics,
        'ablation_settings': {
            'no_disentangled': args.ablation_no_disentangled,
            'no_causal': args.ablation_no_causal,
            'no_quantum': args.ablation_no_quantum,
            'no_multimodal': args.ablation_no_multimodal,
            'text_only': args.ablation_text_only,
            'image_only': args.ablation_image_only
        }
    }

    with open(os.path.join(save_dir, 'results.json'), 'w') as f:
        json.dump(results, f, indent=4)
    
    # TensorBoard: è®°å½•æœ€ç»ˆæµ‹è¯•æŒ‡æ ‡
    if writer:
        for key, value in test_metrics.items():
            writer.add_scalar(f'Test/{key}', value, best_epoch)
        
        # æ·»åŠ è¶…å‚æ•°å’Œæœ€ç»ˆæŒ‡æ ‡
        hparams = {
            'hidden_dim': args.hidden_dim,
            'item_embed_dim': args.item_embed_dim,
            'disentangled_dim': args.disentangled_dim,
            'num_interests': args.num_interests,
            'lr': args.lr,
            'batch_size': args.batch_size,
            'alpha_recon': args.alpha_recon,
            'alpha_causal': args.alpha_causal,
            'alpha_diversity': args.alpha_diversity,
            'alpha_orthogonality': args.alpha_orthogonality
        }
        writer.add_hparams(hparams, {
            'hparam/test_ndcg10': test_metrics['NDCG@10'],
            'hparam/test_hr10': test_metrics['HR@10'],
            'hparam/test_mrr': test_metrics['MRR']
        })
        
        writer.close()
        print(f"âœ“ TensorBoard logs saved to: {os.path.join(args.log_dir, args.exp_name)}")

    print("\n" + "=" * 80)
    print("Training completed!")
    print(f"Experiment: {args.exp_name}")
    print(f"Best epoch: {best_epoch}")
    print(f"Best validation NDCG@10: {checkpoint['valid_metrics']['NDCG@10']:.4f}")
    print(f"Test NDCG@10: {test_metrics['NDCG@10']:.4f}")
    print(f"Test HR@10: {test_metrics['HR@10']:.4f}")
    print(f"Test MRR: {test_metrics['MRR']:.4f}")
    print("=" * 80 + "\n")


if __name__ == '__main__':
    main()
