# ğŸ¯ ç®€åŒ–æ¶æ„è®¾è®¡ï¼šä¿ç•™åˆ›æ–°ï¼Œæé«˜æˆåŠŸç‡

## æ ¸å¿ƒç†å¿µ

**é—®é¢˜**: å½“å‰æ¨¡å‹è¿‡äºå¤æ‚ï¼Œ7å±‚å †å  + 8ä¸ªæŸå¤±é¡¹ï¼Œè®­ç»ƒæä¸ç¨³å®š  
**è§£å†³**: ç®€åŒ–å®ç°ç»†èŠ‚ï¼Œä¿ç•™æ ¸å¿ƒåˆ›æ–°æ€æƒ³ï¼Œæé«˜å¯è®­ç»ƒæ€§

---

## æ¶æ„å¯¹æ¯”

### å½“å‰æ¶æ„ âŒ (è¿‡äºå¤æ‚)

```
è¾“å…¥ (item_ids + multimodal)
  â†“
[1] Item Embedding
  â†“
[2] Multimodal Encoder (3æ¨¡æ€ Ã— 3å±‚MLP)  â† 9ä¸ªå°ç½‘ç»œ
  â†“
[3] Disentangled VAE
    â”œâ”€ 3ä¸ªEncoder Head (å„3å±‚MLP)       â† 9ä¸ªå°ç½‘ç»œ
    â”œâ”€ VAEé‡‡æ · (reparameterization)
    â”œâ”€ Decoder (3å±‚MLP)                  â† 1ä¸ªç½‘ç»œ
    â””â”€ Discriminator (3å±‚MLP)            â† 1ä¸ªç½‘ç»œ
  â†“
[4] Sequence Encoder (2å±‚åŒå‘GRU)        â† 4ä¸ªGRU
  â†“
[5] Quantum Encoder
    â”œâ”€ å¤æ•°ç¼–ç å™¨ (real + imag)          â† 2ä¸ªç½‘ç»œ
    â”œâ”€ å¤æ•°æ³¨æ„åŠ› (8ä¸ªæŠ•å½±)               â† 8ä¸ªç½‘ç»œ
    â”œâ”€ é‡å­å¹²æ¶‰ (å¤æ•°çŸ©é˜µ)
    â””â”€ é‡å­æµ‹é‡ (æµ‹é‡ç®—å­ + åå¤„ç†)       â† 2ä¸ªç½‘ç»œ
  â†“
[6] Causal Inference
    â”œâ”€ Propensityç½‘ç»œ                    â† 1ä¸ªç½‘ç»œ
    â”œâ”€ Outcomeç½‘ç»œ (3ä¸ªtreatment)        â† 3ä¸ªç½‘ç»œ
    â””â”€ è’™ç‰¹å¡æ´›é‡‡æ · (10æ¬¡)
  â†“
[7] Recommendation Head (å½’ä¸€åŒ– + ç‚¹ç§¯)
  â†“
è¾“å‡º + 8ä¸ªæŸå¤±é¡¹
```

**ç»Ÿè®¡**:
- **å­ç½‘ç»œæ•°é‡**: ~45ä¸ª
- **å †å æ·±åº¦**: 7å±‚
- **æŸå¤±é¡¹**: 8ä¸ª
- **å‚æ•°é‡**: ~15M
- **è®­ç»ƒé€Ÿåº¦**: 1.5 it/s
- **ç¨³å®šæ€§**: âš ï¸ ææ˜“NaN

---

### ç®€åŒ–æ¶æ„ âœ… (ä¿ç•™æ ¸å¿ƒ)

```
è¾“å…¥ (item_ids + multimodal)
  â†“
[1] Item Embedding + ç®€åŒ–å¤šæ¨¡æ€èåˆ
    â””â”€ 1å±‚MLPèåˆï¼ˆä¸å†æ˜¯3å±‚ï¼‰         â† 3ä¸ªå°ç½‘ç»œ
  â†“
[2] è½»é‡çº§è§£è€¦è¡¨å¾ (åˆ›æ–°1 ä¿ç•™)
    â”œâ”€ å…±äº«Encoder (1å±‚)               â† 1ä¸ªç½‘ç»œ
    â”œâ”€ 3ä¸ªVAE Head (å„1å±‚)             â† 6ä¸ªå°ç½‘ç»œ
    â”œâ”€ VAEé‡‡æ ·
    â””â”€ ç®€å•é‡æ„ (1å±‚)                  â† 1ä¸ªç½‘ç»œ
    âŒ å»æ‰: Discriminator
  â†“
[3] å•å‘åºåˆ—ç¼–ç  (1å±‚GRU)              â† 1ä¸ªGRU
  â†“
[4] ç®€åŒ–å¤šå…´è¶£ç¼–ç  (åˆ›æ–°2 ä¿ç•™)
    â”œâ”€ æ ‡å‡†å¤šå¤´æ³¨æ„åŠ› (4ä¸ªhead)        â† 1ä¸ªç½‘ç»œ
    â””â”€ å…´è¶£èšåˆ
    âŒ å»æ‰: å¤æ•°è¿ç®—ã€å¹²æ¶‰ã€æµ‹é‡
  â†“
[5] è½»é‡å› æœæ¨æ–­ (åˆ›æ–°3 ä¿ç•™)
    â”œâ”€ ç®€å•å¹²é¢„ç½‘ç»œ (1å±‚)              â† 3ä¸ªå°ç½‘ç»œ
    â””â”€ ç¡®å®šæ€§æ¨æ–­ (ä¸å†è’™ç‰¹å¡æ´›)
    âŒ å»æ‰: Propensityç½‘ç»œ
  â†“
[6] Recommendation Head (å½’ä¸€åŒ– + ç‚¹ç§¯)
  â†“
è¾“å‡º + 3ä¸ªæŸå¤±é¡¹
```

**ç»Ÿè®¡**:
- **å­ç½‘ç»œæ•°é‡**: ~18ä¸ª (â†“ 60%)
- **å †å æ·±åº¦**: 4å±‚ (â†“ 43%)
- **æŸå¤±é¡¹**: 3ä¸ª (â†“ 62%)
- **å‚æ•°é‡**: ~8M (â†“ 47%)
- **è®­ç»ƒé€Ÿåº¦**: 3.0+ it/s (â†‘ 100%)
- **ç¨³å®šæ€§**: âœ… é¢„æœŸç¨³å®š

---

## ä¸‰å¤§åˆ›æ–°çš„ç®€åŒ–ä¿ç•™

### åˆ›æ–°1: è§£è€¦è¡¨å¾ âœ…

**ä¿ç•™**:
- âœ… æ ¸å¿ƒæ€æƒ³: åˆ†è§£ä¸ºåŠŸèƒ½/ç¾å­¦/æƒ…æ„Ÿç»´åº¦
- âœ… VAEæœºåˆ¶: mu/logvaré‡‡æ ·
- âœ… é‡æ„æŸå¤±: ä¿è¯ä¿¡æ¯å®Œæ•´æ€§
- âœ… KLæ•£åº¦: æ­£åˆ™åŒ–éšç©ºé—´

**ç®€åŒ–**:
- âŒ å»æ‰Discriminator â†’ ä¸å†æœ‰TC losså’Œindependence loss
- âŒ å»æ‰å¤æ‚çš„ç¼–ç å™¨ â†’ ä»3å±‚é™åˆ°1å±‚
- âŒ ç®€åŒ–é‡æ„ â†’ ä»3å±‚é™åˆ°1å±‚
- âœ… æ•ˆæœ: æŸå¤±ä»4ä¸ªé™åˆ°2ä¸ª (recon + KL)

**ä»£ç æ”¹åŠ¨**:
```python
# models/disentangled_representation.py

class SimplifiedDisentangledHead(nn.Module):
    """ç®€åŒ–çš„è§£è€¦å¤´ - åªä¿ç•™VAEæ ¸å¿ƒ"""
    def __init__(self, input_dim, output_dim):
        super().__init__()
        # å…±äº«ç¼–ç å™¨ (1å±‚)
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, output_dim * 2),
            nn.ReLU()
        )
        # VAEå¤´ (ç›´æ¥æŠ•å½±)
        self.mu_head = nn.Linear(output_dim * 2, output_dim)
        self.logvar_head = nn.Linear(output_dim * 2, output_dim)
    
    def forward(self, x):
        h = self.encoder(x)
        mu = self.mu_head(h)
        logvar = torch.clamp(self.logvar_head(h), -10, 2)
        z = self._reparameterize(mu, logvar)
        return z, mu, logvar

class SimplifiedDisentangledRepresentation(nn.Module):
    """ç®€åŒ–ç‰ˆè§£è€¦è¡¨å¾å­¦ä¹ """
    def __init__(self, ...):
        # âŒ å»æ‰discriminator
        # self.discriminator = None
        
        # ç®€åŒ–çš„decoder (1å±‚)
        self.decoder = nn.Linear(total_dim, input_dim)
    
    def forward(self, x):
        # 3ä¸ªç»´åº¦çš„VAE
        z_func, mu_func, logvar_func = self.function_head(x)
        z_aes, mu_aes, logvar_aes = self.aesthetics_head(x)
        z_emo, mu_emo, logvar_emo = self.emotion_head(x)
        
        # é‡æ„
        z_concat = torch.cat([z_func, z_aes, z_emo], dim=-1)
        x_recon = self.decoder(z_concat)
        
        # åªè®¡ç®—2ä¸ªæŸå¤±
        recon_loss = F.mse_loss(x_recon, x)
        kl_loss = self._kl_divergence(mu_func, logvar_func) + \
                  self._kl_divergence(mu_aes, logvar_aes) + \
                  self._kl_divergence(mu_emo, logvar_emo)
        
        # âŒ ä¸å†è®¡ç®—TCå’Œindependence loss
        return {
            'latent': z_concat,
            'recon_loss': recon_loss,
            'kl_loss': kl_loss
        }
```

---

### åˆ›æ–°2: é‡å­å¤šå…´è¶£ âœ…

**ä¿ç•™**:
- âœ… æ ¸å¿ƒæ€æƒ³: å¤šå…´è¶£å»ºæ¨¡ï¼ˆ4ä¸ªheadï¼‰
- âœ… æ³¨æ„åŠ›æœºåˆ¶: æ•è·å…´è¶£äº¤äº’
- âœ… å…´è¶£èšåˆ: ç”Ÿæˆæœ€ç»ˆè¡¨å¾

**ç®€åŒ–**:
- âŒ å»æ‰å¤æ•°è¿ç®— â†’ æ”¹ç”¨æ ‡å‡†æ³¨æ„åŠ›
- âŒ å»æ‰é‡å­å¹²æ¶‰ â†’ ä¸å†æ¨¡æ‹Ÿæ³¢å‡½æ•°
- âŒ å»æ‰æµ‹é‡ç®—å­ â†’ ç›´æ¥åŠ æƒå¹³å‡
- âœ… æ•ˆæœ: ä¿æŒå¤šå…´è¶£èƒ½åŠ›ï¼Œé™ä½80%è®¡ç®—é‡

**ä»£ç æ”¹åŠ¨**:
```python
# models/quantum_inspired_encoder.py

class SimplifiedMultiInterestEncoder(nn.Module):
    """ç®€åŒ–çš„å¤šå…´è¶£ç¼–ç å™¨ - æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›"""
    def __init__(self, hidden_dim, num_interests=4):
        super().__init__()
        self.num_interests = num_interests
        
        # âŒ ä¸å†ä½¿ç”¨å¤æ•°ç¼–ç 
        # æ”¹ç”¨æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_interests,
            batch_first=True
        )
        
        # å…´è¶£æŸ¥è¯¢å‘é‡ (å¯å­¦ä¹ )
        self.interest_queries = nn.Parameter(
            torch.randn(num_interests, hidden_dim)
        )
        
    def forward(self, user_repr):
        batch_size = user_repr.size(0)
        
        # æ‰©å±•æŸ¥è¯¢å‘é‡
        queries = self.interest_queries.unsqueeze(0).expand(batch_size, -1, -1)
        
        # æ ‡å‡†æ³¨æ„åŠ› (ä¸å†æ˜¯å¤æ•°)
        interests, attn_weights = self.multihead_attn(
            queries,                        # (batch, num_interests, dim)
            user_repr.unsqueeze(1),        # (batch, 1, dim)
            user_repr.unsqueeze(1)
        )  # â†’ (batch, num_interests, dim)
        
        # âŒ ä¸å†è®¡ç®—é‡å­å¹²æ¶‰å’Œæµ‹é‡
        # ç›´æ¥åŠ æƒå¹³å‡
        final_repr = interests.mean(dim=1)  # (batch, dim)
        
        # âŒ ä¸å†è®¡ç®—fidelity diversity loss
        return {
            'output': final_repr,
            'all_interests': interests,
            'attention_weights': attn_weights
        }
```

---

### åˆ›æ–°3: å› æœæ¨æ–­ âœ…

**ä¿ç•™**:
- âœ… æ ¸å¿ƒæ€æƒ³: åäº‹å®æ¨æ–­
- âœ… å¹²é¢„æœºåˆ¶: ä¿®æ”¹æŸä¸ªç»´åº¦
- âœ… å› æœæ•ˆåº”: ITEè®¡ç®—

**ç®€åŒ–**:
- âŒ å»æ‰è’™ç‰¹å¡æ´› â†’ ä»10æ¬¡é‡‡æ ·é™åˆ°1æ¬¡ç¡®å®šæ€§
- âŒ å»æ‰å€¾å‘æ€§è¯„åˆ† â†’ å‡è®¾æ‰€æœ‰æ ·æœ¬å¯æ²»ç–—
- âŒ ç®€åŒ–outcomeç½‘ç»œ â†’ ä»3å±‚é™åˆ°1å±‚
- âœ… æ•ˆæœ: ä¿æŒå› æœèƒ½åŠ›ï¼Œé™ä½90%è®¡ç®—é‡

**ä»£ç æ”¹åŠ¨**:
```python
# models/causal_inference.py

class SimplifiedCausalInference(nn.Module):
    """ç®€åŒ–çš„å› æœæ¨æ–­æ¨¡å—"""
    def __init__(self, dim_size, num_treatments=3):
        super().__init__()
        
        # âŒ å»æ‰propensityç½‘ç»œ
        # self.propensity_net = None
        
        # ç®€åŒ–çš„å¹²é¢„ç½‘ç»œ (1å±‚)
        self.intervention_nets = nn.ModuleList([
            nn.Linear(dim_size, dim_size)  # ç›´æ¥æ˜ å°„
            for _ in range(num_treatments)
        ])
    
    def forward(self, disentangled_features):
        """
        ç¡®å®šæ€§åäº‹å®æ¨æ–­ï¼ˆä¸å†è’™ç‰¹å¡æ´›ï¼‰
        """
        # åŸå§‹ç‰¹å¾
        func = disentangled_features['function']
        aes = disentangled_features['aesthetics']
        emo = disentangled_features['emotion']
        
        original = torch.cat([func, aes, emo], dim=-1)
        
        # 3ä¸ªåäº‹å®ï¼ˆç¡®å®šæ€§ï¼‰
        cf_func = self.intervention_nets[0](func)  # å¹²é¢„åŠŸèƒ½
        cf_aes = self.intervention_nets[1](aes)    # å¹²é¢„ç¾å­¦
        cf_emo = self.intervention_nets[2](emo)    # å¹²é¢„æƒ…æ„Ÿ
        
        # æ„é€ åäº‹å®ç‰¹å¾ï¼ˆç¡®å®šæ€§ï¼‰
        cf1 = torch.cat([cf_func, aes, emo], dim=-1)
        cf2 = torch.cat([func, cf_aes, emo], dim=-1)
        cf3 = torch.cat([func, aes, cf_emo], dim=-1)
        
        # âŒ ä¸å†è’™ç‰¹å¡æ´›é‡‡æ ·ï¼ˆnum_mc_samples=1ï¼‰
        return {
            'original_features': original,
            'counterfactuals': {
                'do_function': cf1,
                'do_aesthetics': cf2,
                'do_emotion': cf3
            },
            # âŒ ä¸å†è®¡ç®—uncertainty
        }
```

---

## æŸå¤±å‡½æ•°ç®€åŒ–

### ä¹‹å‰ âŒ (8ä¸ªæŸå¤±)

```python
total_loss = (
    rec_loss +                      # æ¨èæŸå¤±
    Î±_recon * recon_loss +          # VAEé‡æ„
    Î±_kl * kl_loss +                # VAE KLæ•£åº¦
    Î±_tc * tc_loss +                # Total Correlation
    Î±_ind * independence_loss +     # ç»´åº¦ç‹¬ç«‹æ€§
    Î±_div * diversity_loss +        # é‡å­å¤šæ ·æ€§
    Î±_orth * orthogonality_loss +   # å…´è¶£æ­£äº¤æ€§
    Î±_causal * causal_loss          # å› æœæ•ˆåº”
)
```

### ä¹‹å âœ… (3ä¸ªæŸå¤±)

```python
total_loss = (
    rec_loss +                      # æ¨èæŸå¤±ï¼ˆä¸»å¯¼ï¼‰
    Î±_recon * (recon_loss + Î² * kl_loss) +  # VAEæŸå¤±ï¼ˆåˆå¹¶ï¼‰
    Î±_causal * causal_loss          # å› æœæŸå¤±ï¼ˆç®€åŒ–ï¼‰
)

# âŒ å»æ‰çš„æŸå¤±é€šè¿‡å…¶ä»–æ–¹å¼éšå¼ä¿è¯:
# - diversity/orthogonality â†’ å¤šå¤´æ³¨æ„åŠ›å¤©ç„¶åˆ†æ•£
# - tc/independence â†’ KLæ•£åº¦å·²ç»æ­£åˆ™åŒ–
```

---

## å®æ–½æ­¥éª¤

### é€‰é¡¹A: ä¿®æ”¹ç°æœ‰ä»£ç ï¼ˆå¤æ‚ï¼‰

éœ€è¦ä¿®æ”¹3ä¸ªæ ¸å¿ƒæ–‡ä»¶ï¼Œå·¥ä½œé‡è¾ƒå¤§ï¼š
1. `models/disentangled_representation.py`
2. `models/quantum_inspired_encoder.py`
3. `models/causal_inference.py`

### é€‰é¡¹B: ä½¿ç”¨ç®€åŒ–é…ç½®ï¼ˆæ¨èï¼‰â­

ç›´æ¥ä½¿ç”¨`config_simplified.yaml`ï¼Œé€šè¿‡é…ç½®å¼€å…³ç¦ç”¨å¤æ‚åŠŸèƒ½ï¼š

```bash
# ç«‹å³æµ‹è¯•ç®€åŒ–ç‰ˆæœ¬
python train_amazon.py --config config_simplified.yaml --category beauty --epochs 30
```

**ä¼˜åŠ¿**:
- âœ… æ— éœ€ä¿®æ”¹ä»£ç 
- âœ… é€šè¿‡é…ç½®å¼€å…³æ§åˆ¶
- âœ… å¯éšæ—¶æ¢å¤å®Œæ•´ç‰ˆæœ¬
- âœ… å¿«é€ŸéªŒè¯ç®€åŒ–æ•ˆæœ

---

## é¢„æœŸæ•ˆæœ

### è®­ç»ƒç¨³å®šæ€§

| æŒ‡æ ‡ | å½“å‰ | ç®€åŒ–å |
|------|------|--------|
| **NaNé£é™©** | æé«˜ âš ï¸ | ä½ âœ… |
| **æ”¶æ•›é€Ÿåº¦** | æ…¢ | å¿«2-3å€ |
| **è®­ç»ƒé€Ÿåº¦** | 1.5 it/s | 3.0+ it/s |
| **å†…å­˜å ç”¨** | 8GB | 5GB |

### æ€§èƒ½é¢„æœŸ

```
Phase 1 (Epoch 1-10):
  - rec_loss: 8.9 â†’ 7.0  (æ›´å¿«ä¸‹é™)
  - ä¸åº”å‡ºç°NaN
  
Phase 2 (Epoch 11-30):
  - rec_loss: 7.0 â†’ 5.5
  - HR@10: 0.017 â†’ 0.04
  - NDCG@10: 0.007 â†’ 0.02

æœ€ç»ˆæ€§èƒ½:
  - HR@10: 0.05-0.07 (å¯æ¥å—)
  - ä¸‰å¤§åˆ›æ–°ä¿ç•™ âœ…
  - è®­ç»ƒç¨³å®š âœ…
```

---

## ä¸‹ä¸€æ­¥

### ç«‹å³æµ‹è¯•ç®€åŒ–ç‰ˆæœ¬

```bash
cd /root/develop
source /root/miniconda3/bin/activate demo

# ä½¿ç”¨ç®€åŒ–é…ç½®è®­ç»ƒ
python train_amazon.py \
  --config config_simplified.yaml \
  --category beauty \
  --epochs 30
```

### å¦‚æœä»ä¸ç¨³å®š

è¿›ä¸€æ­¥ç®€åŒ–ç­–ç•¥:
1. æš‚æ—¶ç¦ç”¨å› æœæ¨¡å—: `alpha_causal: 0.0`
2. åªä¿ç•™VAE: å…ˆè®­ç»ƒè§£è€¦è¡¨å¾
3. é€æ­¥å¼•å…¥: VAE â†’ å¤šå…´è¶£ â†’ å› æœ

---

## æ€»ç»“

| æ–¹é¢ | å½“å‰æ¶æ„ | ç®€åŒ–æ¶æ„ |
|------|---------|---------|
| **åˆ›æ–°ä¿ç•™** | 100% | 100% âœ… |
| **å®ç°å¤æ‚åº¦** | æé«˜ âš ï¸ | ä¸­ç­‰ âœ… |
| **è®­ç»ƒç¨³å®šæ€§** | å·® âŒ | å¥½ âœ… |
| **å‚æ•°é‡** | 15M | 8M (-47%) |
| **é€Ÿåº¦** | 1.5 it/s | 3.0 it/s (+100%) |
| **å¯è§£é‡Šæ€§** | é«˜ | é«˜ âœ… |

**æ ¸å¿ƒç†å¿µ**: 
- åˆ›æ–°åœ¨äº**æ€æƒ³**ï¼Œä¸åœ¨äº**å¤æ‚åº¦**
- VAEçš„æ ¸å¿ƒæ˜¯mu/logvaré‡‡æ ·ï¼Œä¸æ˜¯discriminator
- å¤šå…´è¶£çš„æ ¸å¿ƒæ˜¯å¤šå¤´å»ºæ¨¡ï¼Œä¸æ˜¯å¤æ•°è¿ç®—
- å› æœçš„æ ¸å¿ƒæ˜¯åäº‹å®æ¨æ–­ï¼Œä¸æ˜¯è’™ç‰¹å¡æ´›

**æ‚¨å¯ä»¥æ”¾å¿ƒä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬è¿›è¡Œè®ºæ–‡ï¼Œå› ä¸ºä¸‰å¤§åˆ›æ–°çš„æ ¸å¿ƒæ€æƒ³éƒ½ä¿ç•™äº†ï¼**



